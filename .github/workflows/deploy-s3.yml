name: Deploy S3

on:
  workflow_call:
    inputs:
      # Stack & Repository Configuration
      stack_name:
        description: "CloudFormation stack name"
        required: true
        type: string
      environment:
        description: "Environment to deploy to (prod, staging, or dev)"
        required: true
        type: string

      # GHA Runner Configuration
      runner_config:
        description: "GitHub Actions runner configuration (JSON array)"
        required: false
        type: string
        default: '["ubuntu-latest"]'

      # AWS Configuration
      aws_region:
        description: "AWS region for deployment"
        required: true
        type: string
      
      # Namespace Configuration (auto-computed by caller: main repo = empty, forks = account ID)
      namespace:
        description: "S3 bucket namespace. Main repo uses empty, forks use AWS account ID for uniqueness."
        required: false
        type: string
        default: ""

      # Domain Configuration
      public_domain_name:
        description: "Custom domain for public data CDN (e.g., public.robosystems.ai)"
        required: false
        type: string
        default: ""
      domain_name_root:
        description: "Root domain name for Route 53 hosted zone (e.g., robosystems.ai)"
        required: false
        type: string
        default: ""

    outputs:
      # Policy ARNs
      graph_writer_s3_policy_arn:
        description: "ARN of the graph writer S3 policy"
        value: ${{ jobs.action.outputs.graph_writer_s3_policy_arn }}
      shared_data_writer_policy_arn:
        description: "ARN of the shared data writer S3 policy"
        value: ${{ jobs.action.outputs.shared_data_writer_policy_arn }}

      # Deployment Bucket
      deployment_bucket_name:
        description: "Name of the deployment bucket"
        value: ${{ jobs.action.outputs.deployment_bucket_name }}
      deployment_bucket_arn:
        description: "ARN of the deployment bucket"
        value: ${{ jobs.action.outputs.deployment_bucket_arn }}

      # Shared Raw Bucket
      shared_raw_bucket_name:
        description: "Name of the shared raw data bucket"
        value: ${{ jobs.action.outputs.shared_raw_bucket_name }}
      shared_raw_bucket_arn:
        description: "ARN of the shared raw data bucket"
        value: ${{ jobs.action.outputs.shared_raw_bucket_arn }}

      # Shared Processed Bucket
      shared_processed_bucket_name:
        description: "Name of the shared processed data bucket"
        value: ${{ jobs.action.outputs.shared_processed_bucket_name }}
      shared_processed_bucket_arn:
        description: "ARN of the shared processed data bucket"
        value: ${{ jobs.action.outputs.shared_processed_bucket_arn }}

      # User Data Bucket
      user_data_bucket_name:
        description: "Name of the user data bucket"
        value: ${{ jobs.action.outputs.user_data_bucket_name }}
      user_data_bucket_arn:
        description: "ARN of the user data bucket"
        value: ${{ jobs.action.outputs.user_data_bucket_arn }}

      # Public Data Bucket
      public_data_bucket_name:
        description: "Name of the public data bucket"
        value: ${{ jobs.action.outputs.public_data_bucket_name }}
      public_data_bucket_arn:
        description: "ARN of the public data bucket"
        value: ${{ jobs.action.outputs.public_data_bucket_arn }}
      public_data_cdn_url:
        description: "CloudFront CDN URL for public data"
        value: ${{ jobs.action.outputs.public_data_cdn_url }}

      # Logs Bucket
      logs_bucket_name:
        description: "Name of the logs bucket (Dagster compute logs)"
        value: ${{ jobs.action.outputs.logs_bucket_name }}
      logs_bucket_arn:
        description: "ARN of the logs bucket"
        value: ${{ jobs.action.outputs.logs_bucket_arn }}

    secrets:
      ACTIONS_TOKEN:
        description: "GitHub token for checkout (optional - falls back to github.token)"
        required: false

jobs:
  action:
    runs-on: ${{ fromJSON(inputs.runner_config) }}
    timeout-minutes: 15
    permissions:
      id-token: write
      contents: read
    outputs:
      # Policy ARNs
      graph_writer_s3_policy_arn: ${{ steps.stack-outputs.outputs.graph_writer_s3_policy_arn }}
      shared_data_writer_policy_arn: ${{ steps.stack-outputs.outputs.shared_data_writer_policy_arn }}
      # Deployment Bucket
      deployment_bucket_name: ${{ steps.stack-outputs.outputs.deployment_bucket_name }}
      deployment_bucket_arn: ${{ steps.stack-outputs.outputs.deployment_bucket_arn }}
      # Shared Raw Bucket
      shared_raw_bucket_name: ${{ steps.stack-outputs.outputs.shared_raw_bucket_name }}
      shared_raw_bucket_arn: ${{ steps.stack-outputs.outputs.shared_raw_bucket_arn }}
      # Shared Processed Bucket
      shared_processed_bucket_name: ${{ steps.stack-outputs.outputs.shared_processed_bucket_name }}
      shared_processed_bucket_arn: ${{ steps.stack-outputs.outputs.shared_processed_bucket_arn }}
      # User Data Bucket
      user_data_bucket_name: ${{ steps.stack-outputs.outputs.user_data_bucket_name }}
      user_data_bucket_arn: ${{ steps.stack-outputs.outputs.user_data_bucket_arn }}
      # Public Data Bucket
      public_data_bucket_name: ${{ steps.stack-outputs.outputs.public_data_bucket_name }}
      public_data_bucket_arn: ${{ steps.stack-outputs.outputs.public_data_bucket_arn }}
      public_data_cdn_url: ${{ steps.stack-outputs.outputs.public_data_cdn_url }}
      # Logs Bucket
      logs_bucket_name: ${{ steps.stack-outputs.outputs.logs_bucket_name }}
      logs_bucket_arn: ${{ steps.stack-outputs.outputs.logs_bucket_arn }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: ${{ github.repository }}
          ref: ${{ github.ref }}
          token: ${{ secrets.ACTIONS_TOKEN || github.token }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ vars.AWS_ROLE_ARN }}
          aws-region: ${{ inputs.aws_region }}

      - name: Deploy S3 CloudFormation Stack
        id: deploy-stack
        run: |
          # Look up the hosted zone ID if domain is provided
          if [ -n "${{ inputs.public_domain_name }}" ] && [ -n "${{ inputs.domain_name_root }}" ]; then
            HOSTED_ZONE_ID=$(aws route53 list-hosted-zones | jq -r ".HostedZones[] | select(.Name==\"${{ inputs.domain_name_root }}.\") | .Id" | cut -d'/' -f3)
            
            if [ -z "$HOSTED_ZONE_ID" ]; then
              echo "WARNING: No hosted zone found for domain ${{ inputs.domain_name_root }}, proceeding without custom domain"
              HOSTED_ZONE_ID=""
            else
              echo "Hosted zone configured for domain ${{ inputs.domain_name_root }}"
            fi
          else
            HOSTED_ZONE_ID=""
          fi

          if aws cloudformation describe-stacks --stack-name ${{ inputs.stack_name }} 2>&1 | grep -q 'Stack with id ${{ inputs.stack_name }} does not exist'; then
            STACK_ACTION="create-stack"
            echo "Creating new stack ${{ inputs.stack_name }}"
            echo "is_new_stack=true" >> $GITHUB_OUTPUT
          else
            STACK_ACTION="update-stack"
            echo "Updating existing stack ${{ inputs.stack_name }}"
            echo "is_new_stack=false" >> $GITHUB_OUTPUT
          fi

          S3_STACK_PARAMS="ParameterKey=Environment,ParameterValue=${{ inputs.environment }}"

          # Add namespace parameter if provided (for forks)
          if [ -n "${{ inputs.namespace }}" ]; then
            S3_STACK_PARAMS="$S3_STACK_PARAMS \
                  ParameterKey=Namespace,ParameterValue=${{ inputs.namespace }}"
          fi

          # Add domain parameters if provided
          if [ -n "${{ inputs.public_domain_name }}" ]; then
            S3_STACK_PARAMS="$S3_STACK_PARAMS \
                  ParameterKey=PublicDomainName,ParameterValue=${{ inputs.public_domain_name }}"
          fi
          
          if [ -n "$HOSTED_ZONE_ID" ]; then
            S3_STACK_PARAMS="$S3_STACK_PARAMS \
                  ParameterKey=HostedZoneId,ParameterValue=$HOSTED_ZONE_ID"
          fi

          # Deploy or update the stack
          if [ "$STACK_ACTION" = "create-stack" ]; then
            # Create new stack
            aws cloudformation $STACK_ACTION \
              --stack-name ${{ inputs.stack_name }} \
              --template-body file://cloudformation/s3.yaml \
              --capabilities CAPABILITY_NAMED_IAM \
              --parameters $S3_STACK_PARAMS \
              --tags \
                Key=Environment,Value=${{ inputs.environment }} \
                Key=Service,Value=RoboSystems \
                Key=Component,Value=S3 \
                Key=Repository,Value=${{ github.repository }} \
                Key=CreatedBy,Value=GitHubActions
          else
            # Update existing stack, handling "No updates" error
            UPDATE_OUTPUT=$(aws cloudformation $STACK_ACTION \
              --stack-name ${{ inputs.stack_name }} \
              --template-body file://cloudformation/s3.yaml \
              --capabilities CAPABILITY_NAMED_IAM \
              --parameters $S3_STACK_PARAMS \
              --tags \
                Key=Environment,Value=${{ inputs.environment }} \
                Key=Service,Value=RoboSystems \
                Key=Component,Value=S3 \
                Key=Repository,Value=${{ github.repository }} \
                Key=CreatedBy,Value=GitHubActions 2>&1) || true

            # Check if the error was "No updates are to be performed"
            if echo "$UPDATE_OUTPUT" | grep -q "No updates are to be performed"; then
              echo "Stack is already up to date - no changes needed"
              echo "is_new_stack=false" >> $GITHUB_OUTPUT
            elif echo "$UPDATE_OUTPUT" | grep -q "error"; then
              echo "Error updating stack: $UPDATE_OUTPUT"
              exit 1
            else
              echo "Stack update initiated successfully"
              echo "is_new_stack=false" >> $GITHUB_OUTPUT
            fi
          fi

      - name: Monitor Stack Deployment
        uses: ./.github/actions/monitor-stack-deployment
        with:
          stack-name: ${{ inputs.stack_name }}
          timeout: "1800"
          interval: "10"

      - name: Get Stack Outputs
        id: stack-outputs
        run: |
          echo "Retrieving S3 stack outputs..."

          # Query all stack outputs at once
          S3_OUTPUTS=$(aws cloudformation describe-stacks \
            --stack-name "${{ inputs.stack_name }}" \
            --query 'Stacks[0].Outputs' \
            --output json)

          # Helper function to extract output value
          get_output() {
            echo "$S3_OUTPUTS" | jq -r ".[] | select(.OutputKey==\"$1\") | .OutputValue // empty"
          }

          # Policy ARNs
          echo "graph_writer_s3_policy_arn=$(get_output GraphWriterS3PolicyArn)" >> $GITHUB_OUTPUT
          echo "shared_data_writer_policy_arn=$(get_output SharedDataWriterPolicyArn)" >> $GITHUB_OUTPUT

          # Deployment Bucket
          echo "deployment_bucket_name=$(get_output DeploymentBucketName)" >> $GITHUB_OUTPUT
          echo "deployment_bucket_arn=$(get_output DeploymentBucketArn)" >> $GITHUB_OUTPUT

          # Shared Raw Bucket
          echo "shared_raw_bucket_name=$(get_output SharedRawBucketName)" >> $GITHUB_OUTPUT
          echo "shared_raw_bucket_arn=$(get_output SharedRawBucketArn)" >> $GITHUB_OUTPUT

          # Shared Processed Bucket
          echo "shared_processed_bucket_name=$(get_output SharedProcessedBucketName)" >> $GITHUB_OUTPUT
          echo "shared_processed_bucket_arn=$(get_output SharedProcessedBucketArn)" >> $GITHUB_OUTPUT

          # User Data Bucket
          echo "user_data_bucket_name=$(get_output UserDataBucketName)" >> $GITHUB_OUTPUT
          echo "user_data_bucket_arn=$(get_output UserDataBucketArn)" >> $GITHUB_OUTPUT

          # Public Data Bucket
          echo "public_data_bucket_name=$(get_output PublicDataBucketName)" >> $GITHUB_OUTPUT
          echo "public_data_bucket_arn=$(get_output PublicDataBucketArn)" >> $GITHUB_OUTPUT
          echo "public_data_cdn_url=$(get_output PublicDataCDNURL)" >> $GITHUB_OUTPUT

          # Logs Bucket
          echo "logs_bucket_name=$(get_output LogsBucketName)" >> $GITHUB_OUTPUT
          echo "logs_bucket_arn=$(get_output LogsBucketArn)" >> $GITHUB_OUTPUT

          echo "S3 stack outputs retrieved successfully"

      - name: Update Deployment Status
        if: always()
        run: |
          # Output deployment status for the main workflow
          if [ "${{ steps.deploy-stack.outputs.is_new_stack }}" == "true" ]; then
            echo "S3 stack creation completed"
          else
            echo "S3 stack update completed"
          fi
